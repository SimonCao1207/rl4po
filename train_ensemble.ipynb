{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19fc4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## install finrl library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f939a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75732a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import SP_500_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from settings import *\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "\n",
    "sys.path.append(\"../FinRL-Library\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cd8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICATORS = ['macd',\n",
    "               'rsi_30',\n",
    "               'cci_30',\n",
    "               'dx_30']\n",
    "\n",
    "with open(\"data/stock.json\") as f:\n",
    "    list_tickers = list(json.load(f).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a453d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (251700, 8)\n"
     ]
    }
   ],
   "source": [
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = list_tickers).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a50274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c1b4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 50, State Space: 301\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c5f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.001,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3a7105-02c8-4720-9858-dd22a1647fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fc49048",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window,\n",
    "                 validation_window=validation_window,\n",
    "                 **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0555af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "SAC_model_kwargs = {\n",
    "    \"batch_size\": 64,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "TD3_model_kwargs = {\"batch_size\": 100, \"buffer_size\": 1000000, \"learning_rate\": 0.0001}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000,\n",
    "                 'ppo' : 10_000,\n",
    "                 'ddpg' : 10_000,\n",
    "                 'sac' : 10_000,\n",
    "                 'td3' : 10_000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7c4e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2018-01-02\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_126_2\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0.205     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 25        |\n",
      "|    reward             | 2.0841537 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.246     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 49          |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 5.67        |\n",
      "|    reward             | -0.20682088 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.304       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 247        |\n",
      "|    reward             | -2.4753394 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 31.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.2    |\n",
      "|    explained_variance | 0.0028   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -148     |\n",
      "|    reward             | 0.946813 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 6.8      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 50.3      |\n",
      "|    reward             | 2.8720176 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 104       |\n",
      "|    reward             | -2.324658 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.31      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 54          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.1       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 598         |\n",
      "|    reward             | -0.23419714 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 85.7        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 312      |\n",
      "|    reward             | -2.22329 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 29.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -117       |\n",
      "|    reward             | -1.1651717 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 58         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 86         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 406        |\n",
      "|    reward             | 0.83305067 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 39.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 58        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -0.118    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -107      |\n",
      "|    reward             | 0.5629612 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 58         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 79.4       |\n",
      "|    reward             | -2.2175038 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 58         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 110        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 69.6       |\n",
      "|    reward             | -4.7298675 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 58        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 111       |\n",
      "|    reward             | 4.5280576 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 21.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 404       |\n",
      "|    reward             | 2.2428524 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 45.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 57       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 283      |\n",
      "|    reward             | 1.760967 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 17.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0.0453     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -51.3      |\n",
      "|    reward             | 0.06990004 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -0.00431  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -50.3     |\n",
      "|    reward             | 1.4422376 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 59.3      |\n",
      "|    reward             | 1.7117658 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 173        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 138        |\n",
      "|    reward             | -1.4370869 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.01       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2018-01-02 to  2018-04-04\n",
      "a2c Sharpe Ratio:  -0.03808691703799379\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
      "day: 4027, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2256366.10\n",
      "total_reward: 1256366.10\n",
      "total_cost: 1062.09\n",
      "total_trades: 108768\n",
      "Sharpe: 0.340\n",
      "=================================\n",
      "======ddpg Validation from:  2018-01-02 to  2018-04-04\n",
      "ddpg Sharpe Ratio:  0.042698842804484785\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_126_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 1.24GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2018-01-02 to  2018-04-04\n",
      "td3 Sharpe Ratio:  -0.05505769216932544\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_126_1\n",
      "day: 4027, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3421489.37\n",
      "total_reward: 2421489.37\n",
      "total_cost: 252128.65\n",
      "total_trades: 138462\n",
      "Sharpe: 0.492\n",
      "=================================\n",
      "======sac Validation from:  2018-01-02 to  2018-04-04\n",
      "sac Sharpe Ratio:  -0.027732103563941506\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_126_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 58         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 35         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.21235088 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021793842 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.0247     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.56        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    reward               | -1.3474678  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019086044 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.00764    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.52        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    reward               | -1.7044271  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024673317 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | 0.0138      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.81        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    reward               | -1.5013732  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 21.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019551838 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | 0.0163      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    reward               | 1.2942009   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 41.7        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2018-01-02 to  2018-04-04\n",
      "ppo Sharpe Ratio:  -0.09311324341272155\n",
      "======Best Model Retraining from:  2002-01-01 to  2018-04-04\n",
      "======Trading from:  2018-04-04 to  2018-07-03\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2018-04-04\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_189_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.8      |\n",
      "|    explained_variance | 0.103      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -2.11      |\n",
      "|    reward             | 0.40307775 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 51          |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -70.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 41.9        |\n",
      "|    reward             | -0.38830304 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 0.369       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.7     |\n",
      "|    explained_variance | -0.00133  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 368       |\n",
      "|    reward             | -3.565353 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 34.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 49          |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 40          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -70.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | 60.7        |\n",
      "|    reward             | 0.037745092 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 5.62        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | 0.0775     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -47.3      |\n",
      "|    reward             | 0.82966614 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 7.95       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -63.9      |\n",
      "|    reward             | 0.06890147 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 46           |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 74           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -71          |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | 260          |\n",
      "|    reward             | -0.085160024 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 16.6         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -50.4     |\n",
      "|    reward             | 0.4600365 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 83.6       |\n",
      "|    reward             | 0.34868717 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.84       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 27.6     |\n",
      "|    reward             | 1.396836 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.889    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -134      |\n",
      "|    reward             | 2.8711398 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 18.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 295       |\n",
      "|    reward             | 1.9581777 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 25.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 106        |\n",
      "|    reward             | -4.1341605 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 26.6       |\n",
      "|    reward             | -2.5692925 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 153        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 301        |\n",
      "|    reward             | -0.4582073 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 20.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 119        |\n",
      "|    reward             | -2.8203683 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 15.5       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 49            |\n",
      "|    iterations         | 1700          |\n",
      "|    time_elapsed       | 173           |\n",
      "|    total_timesteps    | 8500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -71.1         |\n",
      "|    explained_variance | -0.0103       |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1699          |\n",
      "|    policy_loss        | 8.78          |\n",
      "|    reward             | -0.0021718587 |\n",
      "|    std                | 1             |\n",
      "|    value_loss         | 0.512         |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 182        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 19.4       |\n",
      "|    reward             | 0.33749336 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 219       |\n",
      "|    reward             | 1.0375577 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 202       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 253       |\n",
      "|    reward             | -1.283637 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 15.1      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2018-04-04 to  2018-07-03\n",
      "a2c Sharpe Ratio:  0.15359684019500633\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
      "day: 4090, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4829342.05\n",
      "total_reward: 3829342.05\n",
      "total_cost: 1781.39\n",
      "total_trades: 98651\n",
      "Sharpe: 0.596\n",
      "=================================\n",
      "======ddpg Validation from:  2018-04-04 to  2018-07-03\n",
      "ddpg Sharpe Ratio:  0.11202018861573568\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_189_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 1.11GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2018-04-04 to  2018-07-03\n",
      "td3 Sharpe Ratio:  0.07641877340852711\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_189_1\n",
      "day: 4090, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5172149.91\n",
      "total_reward: 4172149.91\n",
      "total_cost: 63617.11\n",
      "total_trades: 114726\n",
      "Sharpe: 0.589\n",
      "=================================\n",
      "======sac Validation from:  2018-04-04 to  2018-07-03\n",
      "sac Sharpe Ratio:  0.1012075099090205\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_189_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 53         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 37         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.22314787 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022828197 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    reward               | 0.037703615 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019796818 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.0263     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    reward               | 1.8502791   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 40.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028967276 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.00561    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    reward               | -0.271596   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 25          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021894801 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | -0.0246     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    reward               | -0.4025016  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 34.5        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2018-04-04 to  2018-07-03\n",
      "ppo Sharpe Ratio:  0.10853144089749955\n",
      "======Best Model Retraining from:  2002-01-01 to  2018-07-03\n",
      "======Trading from:  2018-07-03 to  2018-10-02\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2018-07-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_252_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | -0.0317    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 23         |\n",
      "|    reward             | 0.45304257 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 44         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 59.2       |\n",
      "|    reward             | -0.8279618 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.19       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | -0.146     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 497        |\n",
      "|    reward             | -1.5407706 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 59.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -5.64    |\n",
      "|    reward             | 1.76968  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.597    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0.185     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 78.5      |\n",
      "|    reward             | 1.5186206 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.26      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 48          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -76         |\n",
      "|    reward             | -0.78644055 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.15        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 48          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 72          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | -0.342      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 235         |\n",
      "|    reward             | -0.19691853 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 15.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0.019     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -437      |\n",
      "|    reward             | 0.4583672 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 48        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 52.4       |\n",
      "|    reward             | -2.2518802 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0.00501    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 179        |\n",
      "|    reward             | -1.8049448 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.28       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 34.1       |\n",
      "|    reward             | -0.7564875 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.54       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -7.98     |\n",
      "|    reward             | 4.1029572 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.252     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 158      |\n",
      "|    reward             | 2.823253 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 8.8      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 141        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -269       |\n",
      "|    reward             | 0.57219416 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 148       |\n",
      "|    reward             | 3.4092853 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 161       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.5     |\n",
      "|    explained_variance | 1.65e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -179      |\n",
      "|    reward             | -3.366789 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.47      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 171        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.5      |\n",
      "|    explained_variance | 0.182      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -51.6      |\n",
      "|    reward             | -1.4570618 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.32       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 181       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.6     |\n",
      "|    explained_variance | -0.0592   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -36.4     |\n",
      "|    reward             | 1.4342406 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.919     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 191        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 65.9       |\n",
      "|    reward             | -0.5901518 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 201        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -285       |\n",
      "|    reward             | -5.6212163 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 17.3       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2018-07-03 to  2018-10-02\n",
      "a2c Sharpe Ratio:  0.47705753986754057\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
      "day: 4153, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1053010.71\n",
      "total_reward: 53010.71\n",
      "total_cost: 1118.26\n",
      "total_trades: 99699\n",
      "Sharpe: 0.136\n",
      "=================================\n",
      "======ddpg Validation from:  2018-07-03 to  2018-10-02\n",
      "ddpg Sharpe Ratio:  0.057243854470693516\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_252_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 1.15GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2018-07-03 to  2018-10-02\n",
      "td3 Sharpe Ratio:  0.3366501255386006\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_252_1\n",
      "day: 4153, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10651283.55\n",
      "total_reward: 9651283.55\n",
      "total_cost: 243940.33\n",
      "total_trades: 126931\n",
      "Sharpe: 0.823\n",
      "=================================\n",
      "======sac Validation from:  2018-07-03 to  2018-10-02\n",
      "sac Sharpe Ratio:  0.34252243843351243\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_252_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 50         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 40         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.29383782 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022357848 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.00884    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.56        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    reward               | -3.0734508  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020142118 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.0264     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    reward               | 0.45114768  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 28.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028464397 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | 0.00517     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.17        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    reward               | 5.3720293   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026772441 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | 0.00879     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.52        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    reward               | 1.1409357   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 23.3        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2018-07-03 to  2018-10-02\n",
      "ppo Sharpe Ratio:  0.31819226369243864\n",
      "======Best Model Retraining from:  2002-01-01 to  2018-10-02\n",
      "======Trading from:  2018-10-02 to  2019-01-03\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2018-10-02\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_315_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -3.42     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 136       |\n",
      "|    reward             | 0.2138529 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.6       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -1.27      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 106        |\n",
      "|    reward             | -0.3930969 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -0.0176    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 375        |\n",
      "|    reward             | -1.6819671 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 40         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 99.3       |\n",
      "|    reward             | 0.06621076 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0.0107     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 39.5       |\n",
      "|    reward             | 0.34689066 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -0.149     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 13.7       |\n",
      "|    reward             | -1.2076654 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 74         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -0.0159    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 220        |\n",
      "|    reward             | 0.35506877 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 85          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | -0.093      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -49.8       |\n",
      "|    reward             | 0.028103428 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.635       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | -0.831    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -144      |\n",
      "|    reward             | 1.7543445 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0.358     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 86.9      |\n",
      "|    reward             | 0.7807577 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 336       |\n",
      "|    reward             | 0.3650576 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 21.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 115        |\n",
      "|    reward             | -2.3133469 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 136        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | -3.5988076 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.3        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 147         |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 99          |\n",
      "|    reward             | -0.13499147 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.57        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | -0.0241   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 94.3      |\n",
      "|    reward             | 3.2933047 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.34      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 167        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 219        |\n",
      "|    reward             | -0.5953517 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 177        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | -1.12      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 8.55       |\n",
      "|    reward             | -0.9387732 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.42       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 187        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -0.0657    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 10.3       |\n",
      "|    reward             | 0.13115266 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 197        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 34.2       |\n",
      "|    reward             | -1.7584231 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.51       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 48          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 207         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0.0217      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -23.6       |\n",
      "|    reward             | -0.08285461 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.87        |\n",
      "---------------------------------------\n",
      "======a2c Validation from:  2018-10-02 to  2019-01-03\n",
      "a2c Sharpe Ratio:  -0.38073686114794747\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
      "day: 4216, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5970677.03\n",
      "total_reward: 4970677.03\n",
      "total_cost: 1232.17\n",
      "total_trades: 84320\n",
      "Sharpe: 0.656\n",
      "=================================\n",
      "======ddpg Validation from:  2018-10-02 to  2019-01-03\n",
      "ddpg Sharpe Ratio:  -0.35919650633375083\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_315_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.98GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2018-10-02 to  2019-01-03\n",
      "td3 Sharpe Ratio:  -0.40283568055647845\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_315_1\n",
      "day: 4216, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4286481.65\n",
      "total_reward: 3286481.65\n",
      "total_cost: 11478.70\n",
      "total_trades: 106315\n",
      "Sharpe: 0.496\n",
      "=================================\n",
      "======sac Validation from:  2018-10-02 to  2019-01-03\n",
      "sac Sharpe Ratio:  -0.3767310504832917\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_315_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 49          |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 41          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.081442215 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020053256 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | 0.0152      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.69        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0328     |\n",
      "|    reward               | -2.4822643  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017627766 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | 0.00777     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.35        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    reward               | 0.8574499   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 18.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019529346 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.0524     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.09        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    reward               | -0.06981465 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 14.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020296572 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | 0.00801     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.51        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    reward               | 0.50867224  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2018-10-02 to  2019-01-03\n",
      "ppo Sharpe Ratio:  -0.3555490685344637\n",
      "======Best Model Retraining from:  2002-01-01 to  2019-01-03\n",
      "======Trading from:  2019-01-03 to  2019-04-04\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2019-01-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_378_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | -0.976    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 104       |\n",
      "|    reward             | 0.3546517 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 84.1       |\n",
      "|    reward             | 0.48337835 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 406        |\n",
      "|    reward             | -2.6120703 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 43.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0.0465    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 87.9      |\n",
      "|    reward             | 1.3895841 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | -2.74e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -7.09     |\n",
      "|    reward             | 3.5907743 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 14.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.1       |\n",
      "|    explained_variance | 0.031       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 272         |\n",
      "|    reward             | -0.94101596 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 22.3        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 47           |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 73           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -71.1        |\n",
      "|    explained_variance | 0.00338      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | 529          |\n",
      "|    reward             | -0.016658088 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 68.4         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 231       |\n",
      "|    reward             | 1.5750276 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 27.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -3.1      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 238       |\n",
      "|    reward             | 1.4963863 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 15.9      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 105         |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 194         |\n",
      "|    reward             | -0.08677773 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 8.43        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0.0377    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -45       |\n",
      "|    reward             | 1.0535336 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -1.53e+03 |\n",
      "|    reward             | 5.586587  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 493       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -72.5     |\n",
      "|    reward             | 0.7334162 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.72      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 148         |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -382        |\n",
      "|    reward             | 0.062322132 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 39.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 158        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -198       |\n",
      "|    reward             | -3.2586215 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 169        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -22.7      |\n",
      "|    reward             | -7.0387034 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -371      |\n",
      "|    reward             | -1.670618 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 56.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 189       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.5     |\n",
      "|    explained_variance | -0.275    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -67       |\n",
      "|    reward             | 1.9423589 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 200        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -150       |\n",
      "|    reward             | -0.4286395 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.14       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 210         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.5       |\n",
      "|    explained_variance | -1.07       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 17.9        |\n",
      "|    reward             | 0.110124275 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.325       |\n",
      "---------------------------------------\n",
      "======a2c Validation from:  2019-01-03 to  2019-04-04\n",
      "a2c Sharpe Ratio:  0.45835898246086554\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
      "day: 4279, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1545838.81\n",
      "total_reward: 545838.81\n",
      "total_cost: 1206.06\n",
      "total_trades: 118403\n",
      "Sharpe: 0.227\n",
      "=================================\n",
      "======ddpg Validation from:  2019-01-03 to  2019-04-04\n",
      "ddpg Sharpe Ratio:  0.7498327578245683\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_378_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 1.04GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2019-01-03 to  2019-04-04\n",
      "td3 Sharpe Ratio:  0.8610162092466032\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_378_1\n",
      "day: 4279, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6869049.58\n",
      "total_reward: 5869049.58\n",
      "total_cost: 330814.11\n",
      "total_trades: 154784\n",
      "Sharpe: 0.619\n",
      "=================================\n",
      "======sac Validation from:  2019-01-03 to  2019-04-04\n",
      "sac Sharpe Ratio:  0.5773356113491268\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_378_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 51         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 39         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.33275265 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 51         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 79         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02459861 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71        |\n",
      "|    explained_variance   | -0.0166    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 6.2        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    reward               | -3.7909143 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 15.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018638141 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | 0.0118      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    reward               | 0.98361886  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 24.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020916361 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | 0.00368     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    reward               | -0.70680493 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 29          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025627851 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | -0.0636     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    reward               | -5.579806   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-01-03 to  2019-04-04\n",
      "ppo Sharpe Ratio:  0.5185208655757342\n",
      "======Best Model Retraining from:  2002-01-01 to  2019-04-04\n",
      "======Trading from:  2019-04-04 to  2019-07-05\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2019-04-04\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_441_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.8     |\n",
      "|    explained_variance | -0.0429   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 65.7      |\n",
      "|    reward             | 0.4178743 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.937     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 44         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | 0.772      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 85.8       |\n",
      "|    reward             | 0.03132495 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 44         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.8      |\n",
      "|    explained_variance | 0.0617     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 404        |\n",
      "|    reward             | -2.7193375 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 35.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 44         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -69        |\n",
      "|    reward             | -0.5603746 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 2.01       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -70.7    |\n",
      "|    explained_variance | 0.0266   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -69.5    |\n",
      "|    reward             | 2.21614  |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 8.95     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.7      |\n",
      "|    explained_variance | -0.0106    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -173       |\n",
      "|    reward             | -1.0093799 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 17.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 456        |\n",
      "|    reward             | -0.2532311 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 46.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 87        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 30.9      |\n",
      "|    reward             | 1.4143885 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 2.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.8     |\n",
      "|    explained_variance | -0.119    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 208       |\n",
      "|    reward             | 0.2370829 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 13.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | 0.0198     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -84.8      |\n",
      "|    reward             | -0.6512373 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 58.4       |\n",
      "|    reward             | 0.21924962 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.67       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 45          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 130         |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -70.9       |\n",
      "|    explained_variance | 0.0752      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -303        |\n",
      "|    reward             | -0.99695474 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 16.8        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 45          |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 141         |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -70.9       |\n",
      "|    explained_variance | 0.0939      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -147        |\n",
      "|    reward             | -0.94427556 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.07        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -28.4      |\n",
      "|    reward             | -2.8111746 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 4.52       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -116      |\n",
      "|    reward             | 2.3303618 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 5.54      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 174       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.8     |\n",
      "|    explained_variance | -0.000521 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 171       |\n",
      "|    reward             | 1.2153031 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 21.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 186       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.9     |\n",
      "|    explained_variance | 0.0161    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -80.5     |\n",
      "|    reward             | 5.7058315 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 14.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 196        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | -0.679     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -84.4      |\n",
      "|    reward             | 0.48713106 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 2.8        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 45          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 207         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -70.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -100        |\n",
      "|    reward             | -0.47750023 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 2.46        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 217        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | 0.277      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | 0.18608162 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.29       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2019-04-04 to  2019-07-05\n",
      "a2c Sharpe Ratio:  0.15936723385159512\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
      "day: 4342, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5656083.86\n",
      "total_reward: 4656083.86\n",
      "total_cost: 1296.52\n",
      "total_trades: 104351\n",
      "Sharpe: 0.612\n",
      "=================================\n",
      "======ddpg Validation from:  2019-04-04 to  2019-07-05\n",
      "ddpg Sharpe Ratio:  0.14329652610674978\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_441_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.93GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2019-04-04 to  2019-07-05\n",
      "td3 Sharpe Ratio:  0.1909299381987028\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_441_1\n",
      "day: 4342, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5218277.62\n",
      "total_reward: 4218277.62\n",
      "total_cost: 119185.28\n",
      "total_trades: 126544\n",
      "Sharpe: 0.582\n",
      "=================================\n",
      "======sac Validation from:  2019-04-04 to  2019-07-05\n",
      "sac Sharpe Ratio:  0.2045300315612005\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_441_1\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 2048     |\n",
      "| train/             |          |\n",
      "|    reward          | 0.363211 |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029099425 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.0489     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.83        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    reward               | -1.829767   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 9.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019132568 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.0295     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.38        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    reward               | -1.7386504  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025316523 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.0411     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.65        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    reward               | -3.0099404  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 20.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019928092 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | -0.013      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    reward               | -4.247547   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 17.8        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-04-04 to  2019-07-05\n",
      "ppo Sharpe Ratio:  0.27248152425793953\n",
      "======Best Model Retraining from:  2002-01-01 to  2019-07-05\n",
      "======Trading from:  2019-07-05 to  2019-10-03\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2019-07-05\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_504_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | -1.36      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 229        |\n",
      "|    reward             | 0.63088137 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 33.9       |\n",
      "|    reward             | -1.7383032 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.685      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | -0.0328    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 288        |\n",
      "|    reward             | -1.1008642 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 22.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 45          |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 43          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | 12          |\n",
      "|    reward             | -0.18142457 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.9         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 43.6      |\n",
      "|    reward             | 1.8822918 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.45      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -1.48      |\n",
      "|    reward             | -0.6460736 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | -0.0437    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 227        |\n",
      "|    reward             | 0.50697786 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -10.3     |\n",
      "|    reward             | 2.7606337 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -5.57     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 37.4      |\n",
      "|    reward             | 1.1640788 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.1       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 38.7       |\n",
      "|    reward             | 0.07860702 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | -0.553     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -12.2      |\n",
      "|    reward             | -1.1752436 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.465      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 126       |\n",
      "|    reward             | 2.2316334 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 141       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 89.2      |\n",
      "|    reward             | 0.3493452 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.58      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 130       |\n",
      "|    reward             | 1.6892613 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -368       |\n",
      "|    reward             | 0.56714016 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 35.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 174       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 320       |\n",
      "|    reward             | 4.400664  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 22.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -828      |\n",
      "|    reward             | 1.9009534 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 166       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 196      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.4    |\n",
      "|    explained_variance | 0.0235   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -264     |\n",
      "|    reward             | 2.554709 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 15       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 207        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -1.69      |\n",
      "|    reward             | -0.7596522 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 70.1       |\n",
      "|    reward             | 0.61368257 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2019-07-05 to  2019-10-03\n",
      "a2c Sharpe Ratio:  -0.25448149306955914\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
      "day: 4405, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7225819.94\n",
      "total_reward: 6225819.94\n",
      "total_cost: 1070.27\n",
      "total_trades: 96753\n",
      "Sharpe: 0.668\n",
      "=================================\n",
      "======ddpg Validation from:  2019-07-05 to  2019-10-03\n",
      "ddpg Sharpe Ratio:  -0.10655770813391412\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_504_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.89GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2019-07-05 to  2019-10-03\n",
      "td3 Sharpe Ratio:  -0.119293138979897\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_504_1\n",
      "day: 4405, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5650741.23\n",
      "total_reward: 4650741.23\n",
      "total_cost: 269963.29\n",
      "total_trades: 143711\n",
      "Sharpe: 0.580\n",
      "=================================\n",
      "======sac Validation from:  2019-07-05 to  2019-10-03\n",
      "sac Sharpe Ratio:  -0.11522629126938085\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_504_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 50         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 40         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.24680008 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020573704 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.0301     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.98        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0393     |\n",
      "|    reward               | -1.7882437  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020095965 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.033      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.43        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    reward               | 1.2886454   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021914644 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.0135     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.09        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    reward               | -0.510235   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 23.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027901419 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | -0.0551     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.52        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    reward               | -2.188044   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 9.53        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-07-05 to  2019-10-03\n",
      "ppo Sharpe Ratio:  -0.3373062641605581\n",
      "======Best Model Retraining from:  2002-01-01 to  2019-10-03\n",
      "======Trading from:  2019-10-03 to  2020-01-03\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2019-10-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_567_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 98.5       |\n",
      "|    reward             | 0.44411787 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | -0.498     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 57.5       |\n",
      "|    reward             | -0.4065892 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0.0114     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 162        |\n",
      "|    reward             | -0.8861262 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 29.2       |\n",
      "|    reward             | 0.64573663 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.3        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 35.7      |\n",
      "|    reward             | 1.1850003 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.76      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 48          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -29.5       |\n",
      "|    reward             | -0.59725636 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.705       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 48          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 72          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 205         |\n",
      "|    reward             | -0.13908519 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 9.55        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -0.0378   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -33.5     |\n",
      "|    reward             | 0.5791188 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.481     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0.442      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 172        |\n",
      "|    reward             | -1.8993316 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0.117      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -23        |\n",
      "|    reward             | 0.69185704 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 114        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0.00768    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 47.5       |\n",
      "|    reward             | -1.0704596 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -26.3     |\n",
      "|    reward             | 3.9405572 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 43.6      |\n",
      "|    reward             | 0.9175957 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.469     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 78        |\n",
      "|    reward             | 1.9751085 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 155        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -200       |\n",
      "|    reward             | -4.3336196 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -167      |\n",
      "|    reward             | 0.7441108 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.39      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 48          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 176         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 56.9        |\n",
      "|    reward             | -0.75959265 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.874       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 48          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 187         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | -0.573      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 2.94        |\n",
      "|    reward             | -0.66983956 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.24        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 197        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 34.8       |\n",
      "|    reward             | 0.18663757 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 208       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 278       |\n",
      "|    reward             | 0.9103786 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 18.3      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2019-10-03 to  2020-01-03\n",
      "a2c Sharpe Ratio:  0.2190933929799607\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
      "day: 4468, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2189408.79\n",
      "total_reward: 1189408.79\n",
      "total_cost: 1018.58\n",
      "total_trades: 98310\n",
      "Sharpe: 0.308\n",
      "=================================\n",
      "======ddpg Validation from:  2019-10-03 to  2020-01-03\n",
      "ddpg Sharpe Ratio:  0.4876641820122923\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_567_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.80GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2019-10-03 to  2020-01-03\n",
      "td3 Sharpe Ratio:  0.48549424540081954\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_567_1\n",
      "day: 4468, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6430668.64\n",
      "total_reward: 5430668.64\n",
      "total_cost: 422583.69\n",
      "total_trades: 172743\n",
      "Sharpe: 0.628\n",
      "=================================\n",
      "======sac Validation from:  2019-10-03 to  2020-01-03\n",
      "sac Sharpe Ratio:  0.5197375176372234\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_567_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 47         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 43         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.23589683 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 46         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 87         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02438803 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71        |\n",
      "|    explained_variance   | -0.00528   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.69       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    reward               | -3.3878818 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 21.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021118578 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | 0.0121      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    reward               | 1.6477126   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 45          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036178943 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | 0.00864     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    reward               | 0.68344146  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024889935 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.4       |\n",
      "|    explained_variance   | -0.00679    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    reward               | -0.42973492 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2019-10-03 to  2020-01-03\n",
      "ppo Sharpe Ratio:  0.5782246433840822\n",
      "======Best Model Retraining from:  2002-01-01 to  2020-01-03\n",
      "======Trading from:  2020-01-03 to  2020-04-03\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2020-01-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_630_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 36.6      |\n",
      "|    reward             | 1.4048622 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.326     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 48.9       |\n",
      "|    reward             | -1.1828732 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.1        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 225        |\n",
      "|    reward             | -1.3760542 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 17         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -50.2      |\n",
      "|    reward             | 0.36090213 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.24       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 61.1      |\n",
      "|    reward             | 1.9389046 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.43      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -194        |\n",
      "|    reward             | -0.33025295 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 11          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 74         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 115        |\n",
      "|    reward             | -0.8012624 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -42.6     |\n",
      "|    reward             | 1.7519077 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.71      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 330       |\n",
      "|    reward             | 1.3726343 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 33.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 106         |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 189         |\n",
      "|    reward             | -0.69072145 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 8.54        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 47           |\n",
      "|    iterations         | 1100         |\n",
      "|    time_elapsed       | 116          |\n",
      "|    total_timesteps    | 5500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -71.2        |\n",
      "|    explained_variance | -0.234       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1099         |\n",
      "|    policy_loss        | -18.4        |\n",
      "|    reward             | -0.034965366 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 1.56         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -116      |\n",
      "|    reward             | 1.8770534 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 135        |\n",
      "|    reward             | -1.9059204 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | -4.29e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 493       |\n",
      "|    reward             | -2.020421 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 66.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0.0103    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -299      |\n",
      "|    reward             | 4.9449167 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 25.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 170        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 732        |\n",
      "|    reward             | -0.1392432 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 115        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 181        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 599        |\n",
      "|    reward             | -1.4408069 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 93.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 191        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 124        |\n",
      "|    reward             | -7.4654183 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.36       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 201        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 138        |\n",
      "|    reward             | -1.1632584 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 212       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -87       |\n",
      "|    reward             | -3.108361 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.05      |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2020-01-03 to  2020-04-03\n",
      "a2c Sharpe Ratio:  -0.30342279056346505\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_1\n",
      "day: 4531, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8610321.20\n",
      "total_reward: 7610321.20\n",
      "total_cost: 1079.91\n",
      "total_trades: 90670\n",
      "Sharpe: 0.701\n",
      "=================================\n",
      "======ddpg Validation from:  2020-01-03 to  2020-04-03\n",
      "ddpg Sharpe Ratio:  -0.39357991913640855\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_630_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.79GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2020-01-03 to  2020-04-03\n",
      "td3 Sharpe Ratio:  -0.4359877916562993\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_630_1\n",
      "day: 4531, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7080569.24\n",
      "total_reward: 6080569.24\n",
      "total_cost: 31746.81\n",
      "total_trades: 131123\n",
      "Sharpe: 0.609\n",
      "=================================\n",
      "======sac Validation from:  2020-01-03 to  2020-04-03\n",
      "sac Sharpe Ratio:  -0.4438640336563768\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_630_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 49         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 41         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.29898492 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018081456 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.39        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    reward               | -2.7388124  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021222841 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.00431    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.81        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    reward               | -0.4043632  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024104342 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | 0.0014      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.07        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    reward               | -0.42401978 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 20.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022031927 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.012      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 35.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    reward               | 0.6432358   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 70.5        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2020-01-03 to  2020-04-03\n",
      "ppo Sharpe Ratio:  -0.41539519729793906\n",
      "======Best Model Retraining from:  2002-01-01 to  2020-04-03\n",
      "======Trading from:  2020-04-03 to  2020-07-06\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2020-04-03\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_693_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 150       |\n",
      "|    reward             | 0.6318895 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.51      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0.0298     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 119        |\n",
      "|    reward             | -2.0600777 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 301        |\n",
      "|    reward             | 0.35445112 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 27.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -107      |\n",
      "|    reward             | 2.4037566 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 65.3      |\n",
      "|    reward             | 1.2750076 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -57.6     |\n",
      "|    reward             | -0.878867 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.53      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 125        |\n",
      "|    reward             | 0.06897363 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -0.242     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -10.9      |\n",
      "|    reward             | 0.70630956 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.311      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.3    |\n",
      "|    explained_variance | 0.0266   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 344      |\n",
      "|    reward             | 7.033335 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 27.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0.302     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -127      |\n",
      "|    reward             | 0.7512978 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.6       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -1.2604556 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.12       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 45          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 130         |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -710        |\n",
      "|    reward             | -0.43524823 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 91.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 141       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 92.2      |\n",
      "|    reward             | 1.3671612 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -67       |\n",
      "|    reward             | 1.6455506 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.67      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 255       |\n",
      "|    reward             | 1.5559906 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 15.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 18.6       |\n",
      "|    reward             | 0.05938012 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.03       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 185      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.4    |\n",
      "|    explained_variance | -0.0033  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -154     |\n",
      "|    reward             | 2.395241 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 27       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 195      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.4    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -234     |\n",
      "|    reward             | 2.764779 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 13.7     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 46           |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 206          |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -71.4        |\n",
      "|    explained_variance | 0.0952       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 181          |\n",
      "|    reward             | -0.034162335 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 13.2         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 216        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -194       |\n",
      "|    reward             | 0.16180508 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.99       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2020-04-03 to  2020-07-06\n",
      "a2c Sharpe Ratio:  0.2962324331790841\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_1\n",
      "day: 4594, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1820065.98\n",
      "total_reward: 820065.98\n",
      "total_cost: 1084.53\n",
      "total_trades: 137865\n",
      "Sharpe: 0.258\n",
      "=================================\n",
      "======ddpg Validation from:  2020-04-03 to  2020-07-06\n",
      "ddpg Sharpe Ratio:  0.3468225423045298\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_693_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.72GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2020-04-03 to  2020-07-06\n",
      "td3 Sharpe Ratio:  0.36946655510673343\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_693_1\n",
      "day: 4594, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3394044.17\n",
      "total_reward: 2394044.17\n",
      "total_cost: 389267.05\n",
      "total_trades: 157020\n",
      "Sharpe: 0.428\n",
      "=================================\n",
      "======sac Validation from:  2020-04-03 to  2020-07-06\n",
      "sac Sharpe Ratio:  0.38224484746085996\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_693_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 45         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 44         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.24392839 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023791779 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.0171     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.91        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    reward               | -2.4975238  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014936523 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.0162     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.75        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    reward               | -3.0062752  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021956068 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.00413    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    reward               | 0.90534925  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 47         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 216        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02124551 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71.3      |\n",
      "|    explained_variance   | 0.00063    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 10.2       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0457    |\n",
      "|    reward               | -0.8958246 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 15.4       |\n",
      "----------------------------------------\n",
      "======ppo Validation from:  2020-04-03 to  2020-07-06\n",
      "ppo Sharpe Ratio:  0.24952429404845045\n",
      "======Best Model Retraining from:  2002-01-01 to  2020-07-06\n",
      "======Trading from:  2020-07-06 to  2020-10-02\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2020-07-06\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_756_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 9.05      |\n",
      "|    reward             | 2.1434143 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.564     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 44          |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71         |\n",
      "|    explained_variance | -0.295      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | -43.8       |\n",
      "|    reward             | -0.26252994 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.477       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0.0494    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 279       |\n",
      "|    reward             | -1.075125 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 27.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 44       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -70.9    |\n",
      "|    explained_variance | -0.318   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -102     |\n",
      "|    reward             | 2.056762 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 44       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71      |\n",
      "|    explained_variance | -0.107   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 98.2     |\n",
      "|    reward             | 2.404096 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 5.47     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 44         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | 0.0297     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -3.99      |\n",
      "|    reward             | -1.0784041 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.71       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.9     |\n",
      "|    explained_variance | 0.159     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 369       |\n",
      "|    reward             | 2.8660693 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 34.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 44       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -70.9    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -85.2    |\n",
      "|    reward             | 6.208108 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 3.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 44       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -70.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -327     |\n",
      "|    reward             | -0.86458 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 29.6     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 44          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 111         |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -70.9       |\n",
      "|    explained_variance | 0.00774     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 196         |\n",
      "|    reward             | -0.50302064 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 13.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -145      |\n",
      "|    reward             | 2.4308434 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.24      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 61.7      |\n",
      "|    reward             | 2.2098548 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -98.6     |\n",
      "|    reward             | 1.5166144 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.7       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -101      |\n",
      "|    reward             | 7.1056023 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 165        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 389        |\n",
      "|    reward             | -0.1062854 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 41.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | -0.00269   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -237       |\n",
      "|    reward             | -5.0779834 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 187       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 1.54e+03  |\n",
      "|    reward             | 7.2631807 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 587       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 198       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 1.18e+03  |\n",
      "|    reward             | 4.8723063 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 283       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 45         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 209        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0.0542     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -226       |\n",
      "|    reward             | 0.11016371 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 220       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -6.05     |\n",
      "|    reward             | 1.5261462 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.0639    |\n",
      "-------------------------------------\n",
      "======a2c Validation from:  2020-07-06 to  2020-10-02\n",
      "a2c Sharpe Ratio:  0.1639760564729303\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_756_1\n",
      "day: 4657, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4979093.25\n",
      "total_reward: 3979093.25\n",
      "total_cost: 1527.99\n",
      "total_trades: 130515\n",
      "Sharpe: 0.509\n",
      "=================================\n",
      "======ddpg Validation from:  2020-07-06 to  2020-10-02\n",
      "ddpg Sharpe Ratio:  0.25556496971273834\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_756_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.66GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2020-07-06 to  2020-10-02\n",
      "td3 Sharpe Ratio:  0.11008147491160619\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_756_1\n",
      "day: 4657, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2870161.33\n",
      "total_reward: 1870161.33\n",
      "total_cost: 42509.68\n",
      "total_trades: 131100\n",
      "Sharpe: 0.361\n",
      "=================================\n",
      "======sac Validation from:  2020-07-06 to  2020-10-02\n",
      "sac Sharpe Ratio:  0.15957228105579607\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_756_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 44        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.5108159 |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019281374 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.000426   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    reward               | -3.069233   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 17.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 136        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01757497 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71        |\n",
      "|    explained_variance   | 0.00981    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 11.9       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0325    |\n",
      "|    reward               | 2.1855261  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 29.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027198995 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | 0.00253     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 32          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    reward               | -2.880274   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 94          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023922002 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.0172     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.95        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    reward               | -0.21040516 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2020-07-06 to  2020-10-02\n",
      "ppo Sharpe Ratio:  0.1848525894528125\n",
      "======Best Model Retraining from:  2002-01-01 to  2020-10-02\n",
      "======Trading from:  2020-10-02 to  2021-01-04\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2020-10-02\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_819_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -0.885     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -2.92      |\n",
      "|    reward             | 0.33024797 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.3        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 93.2      |\n",
      "|    reward             | 0.1598369 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.84      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 286        |\n",
      "|    reward             | -2.8136048 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 24.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -28.8     |\n",
      "|    reward             | 1.8502352 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.771     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -0.271    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 55.1      |\n",
      "|    reward             | 1.7477218 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -3.76      |\n",
      "|    reward             | -0.9081207 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.991      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 34.1       |\n",
      "|    reward             | -0.9502878 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.68       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 178       |\n",
      "|    reward             | 1.3258787 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.82      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 579       |\n",
      "|    reward             | 3.1440237 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 73.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 43          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 115         |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -38.5       |\n",
      "|    reward             | -0.80955434 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.03        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -77.3     |\n",
      "|    reward             | 1.2887028 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.54      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -66.2      |\n",
      "|    reward             | -1.7432597 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | -0.266    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -329      |\n",
      "|    reward             | 3.9514291 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 24        |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 43          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 161         |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.3       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 249         |\n",
      "|    reward             | -0.19773215 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 14.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 173       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0.0761    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 184       |\n",
      "|    reward             | 1.2056906 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.72      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 43          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 185         |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -27.5       |\n",
      "|    reward             | -0.29889357 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.73        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 31.4       |\n",
      "|    reward             | 0.35075966 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.67       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 43          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 206         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -976        |\n",
      "|    reward             | -0.28384605 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 202         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 217       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | -0.68     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -124      |\n",
      "|    reward             | 0.6709348 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.38      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 228        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -0.0314    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -195       |\n",
      "|    reward             | -3.1566842 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 15.6       |\n",
      "--------------------------------------\n",
      "======a2c Validation from:  2020-10-02 to  2021-01-04\n",
      "a2c Sharpe Ratio:  0.44516073363049863\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_819_1\n",
      "day: 4720, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2794527.23\n",
      "total_reward: 1794527.23\n",
      "total_cost: 1141.70\n",
      "total_trades: 122330\n",
      "Sharpe: 0.351\n",
      "=================================\n",
      "======ddpg Validation from:  2020-10-02 to  2021-01-04\n",
      "ddpg Sharpe Ratio:  0.41254774005003253\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_819_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.66GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2020-10-02 to  2021-01-04\n",
      "td3 Sharpe Ratio:  0.46618150870654795\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_819_1\n",
      "day: 4720, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6862744.34\n",
      "total_reward: 5862744.34\n",
      "total_cost: 126508.01\n",
      "total_trades: 133569\n",
      "Sharpe: 0.537\n",
      "=================================\n",
      "======sac Validation from:  2020-10-02 to  2021-01-04\n",
      "sac Sharpe Ratio:  0.16263423156652954\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_819_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 44         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 45         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.05906761 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027110012 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.0371     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.39        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    reward               | -2.3961272  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024805754 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.1       |\n",
      "|    explained_variance   | -0.0194     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.18        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    reward               | -3.7688298  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 17.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028535549 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | 0.00919     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    reward               | 3.594653    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 58.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 227        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01935294 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71.2      |\n",
      "|    explained_variance   | 0.0266     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 14.8       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    reward               | 0.9704434  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 35.6       |\n",
      "----------------------------------------\n",
      "======ppo Validation from:  2020-10-02 to  2021-01-04\n",
      "ppo Sharpe Ratio:  0.20426331669901557\n",
      "======Best Model Retraining from:  2002-01-01 to  2021-01-04\n",
      "======Trading from:  2021-01-04 to  2021-04-06\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2021-01-04\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_882_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 113       |\n",
      "|    reward             | 0.3046375 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.81      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -70.9     |\n",
      "|    explained_variance | -0.00432  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 254       |\n",
      "|    reward             | 2.2809105 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 471        |\n",
      "|    reward             | -3.4048052 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 84.1       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 43       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -70.9    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 101      |\n",
      "|    reward             | 4.38526  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 10.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -10.3     |\n",
      "|    reward             | 6.7994766 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 60.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -70.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 23.1       |\n",
      "|    reward             | -0.7351788 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 734        |\n",
      "|    reward             | 0.59185135 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 130        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 43       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -323     |\n",
      "|    reward             | 2.350597 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 37.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 486       |\n",
      "|    reward             | 6.0379286 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 121       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 44          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 113         |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 73.2        |\n",
      "|    reward             | -0.32163745 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.37        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -189      |\n",
      "|    reward             | 1.3514442 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.63      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -157       |\n",
      "|    reward             | -1.7523191 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | -0.00367   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -780       |\n",
      "|    reward             | -11.707056 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 329        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 43        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 313       |\n",
      "|    reward             | 4.250182  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 23.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 43          |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 170         |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 64.1        |\n",
      "|    reward             | -0.40963724 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.13        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 43         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 182        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -770       |\n",
      "|    reward             | -7.0540996 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 170        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 43       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 193      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 748      |\n",
      "|    reward             | 3.031058 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 101      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 204       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 722       |\n",
      "|    reward             | 17.319824 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 129       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 44        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 215       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -720      |\n",
      "|    reward             | 21.132841 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 160       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 44          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 226         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -58.3       |\n",
      "|    reward             | -0.34573376 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.37        |\n",
      "---------------------------------------\n",
      "======a2c Validation from:  2021-01-04 to  2021-04-06\n",
      "a2c Sharpe Ratio:  0.27647619278734553\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_882_1\n",
      "day: 4783, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6177569.63\n",
      "total_reward: 5177569.63\n",
      "total_cost: 4420.42\n",
      "total_trades: 120517\n",
      "Sharpe: 0.575\n",
      "=================================\n",
      "======ddpg Validation from:  2021-01-04 to  2021-04-06\n",
      "ddpg Sharpe Ratio:  0.34377785900332\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_882_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.58GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2021-01-04 to  2021-04-06\n",
      "td3 Sharpe Ratio:  0.2668946462825803\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_882_1\n",
      "day: 4783, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6169029.00\n",
      "total_reward: 5169029.00\n",
      "total_cost: 444110.88\n",
      "total_trades: 186726\n",
      "Sharpe: 0.567\n",
      "=================================\n",
      "======sac Validation from:  2021-01-04 to  2021-04-06\n",
      "sac Sharpe Ratio:  0.2018262805599679\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_882_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 43         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 46         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.46874663 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025339168 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71         |\n",
      "|    explained_variance   | -0.0134     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.77        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    reward               | -1.2478311  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 12.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 44         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 138        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02240261 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71.1      |\n",
      "|    explained_variance   | -0.0438    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.59       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    reward               | 0.92028666 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 17         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02349814  |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.00315    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    reward               | -0.53853637 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 56.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025039691 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.3       |\n",
      "|    explained_variance   | 0.0224      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.32        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    reward               | 2.1096373   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2021-01-04 to  2021-04-06\n",
      "ppo Sharpe Ratio:  0.22986162204572028\n",
      "======Best Model Retraining from:  2002-01-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  330.749240787695\n",
      "======Model training from:  2002-01-01 to  2021-04-06\n",
      "======a2c Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_945_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 41       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.2    |\n",
      "|    explained_variance | -0.0995  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 36       |\n",
      "|    reward             | 0.464875 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.812    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 12.7       |\n",
      "|    reward             | -0.3289299 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.307      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | -0.0208    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 296        |\n",
      "|    reward             | -0.7498292 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 26.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0.151     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -67.1     |\n",
      "|    reward             | 1.1718475 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | -0.00792  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 125       |\n",
      "|    reward             | 1.3580877 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 53.7       |\n",
      "|    reward             | -1.5110965 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.71       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 42          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 81          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.3       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 393         |\n",
      "|    reward             | -0.13834558 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 35.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 1.78      |\n",
      "|    reward             | 1.4015722 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.82      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 42       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 105      |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 254      |\n",
      "|    reward             | 4.034671 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 17       |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 42          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 117         |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -71.3       |\n",
      "|    explained_variance | 0.262       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -330        |\n",
      "|    reward             | -0.71328276 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 24.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 81.9       |\n",
      "|    reward             | -0.7514001 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 18.2       |\n",
      "|    reward             | -1.5959679 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 245        |\n",
      "|    reward             | -1.2711406 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 49.4      |\n",
      "|    reward             | 2.0798745 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 175       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -50       |\n",
      "|    reward             | 1.1716648 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.923     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 187       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -1.87     |\n",
      "|    reward             | 0.5099926 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 199        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 176        |\n",
      "|    reward             | -0.4322625 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 42         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 211        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -71.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -27.3      |\n",
      "|    reward             | -0.5175202 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.888      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 42        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 222       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -71.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 302       |\n",
      "|    reward             | -8.385627 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 51.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 42       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 234      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -71.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 21.9     |\n",
      "|    reward             | 1.507894 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.51     |\n",
      "------------------------------------\n",
      "======a2c Validation from:  2021-04-06 to  2021-07-06\n",
      "a2c Sharpe Ratio:  0.0819606137726771\n",
      "======ddpg Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_945_1\n",
      "day: 4846, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11450966.78\n",
      "total_reward: 10450966.78\n",
      "total_cost: 1077.04\n",
      "total_trades: 116304\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "======ddpg Validation from:  2021-04-06 to  2021-07-06\n",
      "ddpg Sharpe Ratio:  0.4373484463494407\n",
      "======td3 Training========\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/td3/td3_945_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hainam/miniconda3/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.62GB > 0.50GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======td3 Validation from:  2021-04-06 to  2021-07-06\n",
      "td3 Sharpe Ratio:  0.6035980634900817\n",
      "======sac Training========\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/sac/sac_945_1\n",
      "day: 4846, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 12496611.84\n",
      "total_reward: 11496611.84\n",
      "total_cost: 363856.64\n",
      "total_trades: 153832\n",
      "Sharpe: 0.701\n",
      "=================================\n",
      "======sac Validation from:  2021-04-06 to  2021-07-06\n",
      "sac Sharpe Ratio:  0.30264427969567476\n",
      "======ppo Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_945_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 42         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 48         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.42161074 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 42         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01726789 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71        |\n",
      "|    explained_variance   | 0.00226    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 6.26       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    reward               | -1.0871559 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 14.5       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 42         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 145        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01877011 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71        |\n",
      "|    explained_variance   | -0.0268    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 8.67       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    reward               | 0.3853853  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 19.1       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 42         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 190        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03398712 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71.1      |\n",
      "|    explained_variance   | 0.00961    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 22.5       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    reward               | -2.9727964 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 61.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021327887 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -71.2       |\n",
      "|    explained_variance   | -0.0547     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.99        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    reward               | 0.44279405  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 20.4        |\n",
      "-----------------------------------------\n",
      "======ppo Validation from:  2021-04-06 to  2021-07-06\n",
      "ppo Sharpe Ratio:  0.25721064919834513\n",
      "======Best Model Retraining from:  2002-01-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "Ensemble Strategy took:  387.6847772200902  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 SAC_model_kwargs,\n",
    "                                                 TD3_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
